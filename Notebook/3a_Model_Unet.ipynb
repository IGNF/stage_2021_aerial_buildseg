{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e28deb4",
   "metadata": {},
   "source": [
    "# Model UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a765aa3",
   "metadata": {},
   "source": [
    "**Objectif:** le but de ce notebook est d'expliquer le modèle UNet et ses différentes méthodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1eb01",
   "metadata": {},
   "source": [
    "### Root Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "646ecc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b1ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/ign.fr/ttea/Code_IGN/AerialImageDataset'\n",
    "train_dir = os.path.join(root,'train/images')\n",
    "gt_dir = os.path.join(root,'train/gt')\n",
    "test_dir = os.path.join(root,'test/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c91992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eeaa75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/home/ign.fr/ttea/stage_segmentation_2021/Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f5ed9",
   "metadata": {},
   "source": [
    "### Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af58ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pandas as pd \n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03f05fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.dataloader import InriaDataset\n",
    "from model.model import UNet\n",
    "from train import train, eval, train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c27de3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "var= pd.read_json('variables.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2b378",
   "metadata": {},
   "source": [
    "### Inria Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979860a0",
   "metadata": {},
   "source": [
    "Définition de la partie train et validation du jeu de données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51be1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = (250,250)\n",
    "train_dataset = InriaDataset(var['variables']['root'],tile_size,'train',None,False,1)\n",
    "val_dataset = InriaDataset(var['variables']['root'],tile_size,'validation',None,False,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99b72c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 250, 250])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bce585",
   "metadata": {},
   "source": [
    "## U-Net Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fae094",
   "metadata": {},
   "source": [
    "### Convolution Block "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da85e9d",
   "metadata": {},
   "source": [
    "La fonction conv_bloc va prendre en entrée le nombre de canal en entrée et le nombre de canal en sortie. Elle va retourner un block de deux opérations de convolutions 3x3 chacune suivi de la fonction d'activation ReLU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78f859b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double Conv2D\n",
    "def conv_block(in_channel, out_channel):\n",
    "    \"\"\"\n",
    "    in_channel : number of input channel, int \n",
    "    out_channel : number of output channel, int\n",
    "    \n",
    "    Returns : Conv Block of 2x Conv2D with ReLU \n",
    "    \"\"\"\n",
    "    \n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_channel, out_channel, kernel_size=3,padding=1),\n",
    "        nn.ReLU(inplace= True),\n",
    "        nn.Conv2d(out_channel, out_channel, kernel_size=3,padding=1),\n",
    "        nn.ReLU(inplace= True),\n",
    "    )\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d2359",
   "metadata": {},
   "source": [
    "### Crop "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95738d1",
   "metadata": {},
   "source": [
    "La fonction crop est une fonction de recadrage, son objectif est de recadrer les tenseurs pour qu'ils soient de même taille car certains peuvent perdre des pixels au niveau des bordures dues aux opérations de convolutions. \n",
    "\n",
    "La fonction crop prend en entrée 2 tenseurs:\n",
    "\n",
    "- Tenseur base \n",
    "- Tenseur à recadrer\n",
    "\n",
    "Pour le recadrage, on se basera sur la taille du tenseur de base et on va ainsi recadrer l'autre tenseur.\n",
    "\n",
    "On va mesurer l'écart entre les tailles des 2 tenseurs (delta), puis on distinguera 2 cas :\n",
    "\n",
    "- pair : Si la taille du tenseur à recadrer - delta  est divisible par 2\n",
    "- impair : Si la taille du tenseur à recadrer - delta  n'est pas divisible par 2\n",
    "\n",
    "On retournera ensuite le tenseur recadré. \n",
    "\n",
    "**Exemple :** \n",
    "\n",
    "**tile size : (250,250)**\n",
    "\n",
    "On affiche la taille des tenseurs par blocs : \n",
    "\n",
    "**block 1:** \n",
    "\n",
    "- u6 torch.Size([1, 128, 30, 30])\n",
    "- c4 torch.Size([1, 128, 31, 31])\n",
    "\n",
    "**block 2:**\n",
    "- u7 torch.Size([1, 64, 60, 60])\n",
    "- c3 torch.Size([1, 64, 62, 62])\n",
    "\n",
    "**block 3:**\n",
    "- u8 torch.Size([1, 32, 120, 120])\n",
    "- c2 torch.Size([1, 32, 125, 125])\n",
    "\n",
    "**block 4:**\n",
    "- u9 torch.Size([1, 16, 240, 240])\n",
    "- c1 torch.Size([1, 16, 250, 250])\n",
    "\n",
    "On a donc besoin de recadrer chaque tenseur.\n",
    "\n",
    "Pour le block 4:\n",
    "target_tensor_size = 240\n",
    "tensor_size =  250 \n",
    "delta = 10 \n",
    "d = delta // 2 = 5\n",
    "\n",
    "On voit que 240- 10 % 2 = 0\n",
    "\n",
    "On est dans le cas pair : \n",
    "- On va recadrer le tenseur en ne prenant que tenseur[:,:, d:240 -d , d: 240-d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9c501d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(target_tensor, tensor): # x,c\n",
    "    \"\"\"\n",
    "    target_tensor : target the tensor to crop  \n",
    "    tensor: tensor \n",
    "    \n",
    "    Returns : tensor cropped by half left side image concatenate with right side image\n",
    "    \n",
    "    \"\"\"\n",
    "       \n",
    "    target_size = target_tensor.size()[2] \n",
    "    tensor_size = tensor.size()[2]        \n",
    "    delta = tensor_size - target_size     \n",
    "    delta = delta // 2                    \n",
    "    \n",
    "    if (tensor_size - 2*delta)%2 == 0:\n",
    "      tens = tensor[:, :, delta:tensor_size- delta , delta:tensor_size-delta]\n",
    "\n",
    "    elif (tensor_size -2*delta)%2 ==1:\n",
    "      tens = tensor[:, :, delta:tensor_size- delta -1  , delta:tensor_size-delta -1]\n",
    "    return tens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380cefa",
   "metadata": {},
   "source": [
    "### UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d9c6f",
   "metadata": {},
   "source": [
    "![title](../img/Unet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6b911",
   "metadata": {},
   "source": [
    "Le modèle UNet est de type encodeur-décodeur. \n",
    "\n",
    "Dans la première partie, on voit que l'encodeur suit une architecture typique d'un réseau de neurones convolutif. Le réseau consiste en une application répétée de 2 convolutions 3x3 chacune suivi de la fonction d'activation ReLU & d'une opération de Maxpooling (pour le downsampling). A chaque étape de sous-échantillonnage, on double le nombre de canaux.\n",
    "\n",
    "Dans la seconde partie, chaque étape dans le décodeur consiste à un suréchantillonnage (upsampling), de la carte de caractéristiques suivi d'une convolution 2x2. Cela aura pour but de diviser par 2 le nombre de canaux. On va ensuite faire une opération de concatenation avec la feature map rognée par rapport à l'encodeur et une opération 3x3 de convolution suivi d'une ReLU. \n",
    "\n",
    "Ensuite, le recadrage est nécessaire à cause de la perte de pixels de bordure dans chaque convolution.\n",
    "Au niveau de la couche finale, une opération de convolution 1x1 est utilisée pour mapper chaque vecteur d’entités à 64 composants au nombre de classes souhaité. Le réseau est donc composé de 23 couches convolutives. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f15ecd",
   "metadata": {},
   "source": [
    "Lien de publication : https://arxiv.org/pdf/1505.04597.pdf\n",
    "\n",
    "Pour mieux comprendre : https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1fce96",
   "metadata": {},
   "source": [
    "Il est possible d'ajuster le modèle UNet pour avoir un code plus modulaire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e02b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "  \"\"\"\n",
    "  UNet network for semantic segmentation\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, n_channels, n_class, cuda = 1):\n",
    "    \"\"\"\n",
    "    initialization function\n",
    "    n_channels, int, number of input channel\n",
    "    conv_width, int list, depth of the convs\n",
    "    n_class = int,  the number of classes\n",
    "    \"\"\"\n",
    "    super(UNet, self).__init__() #necessary for all classes extending the module class\n",
    "    self.is_cuda = cuda\n",
    "    self.n_class = n_class\n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    \n",
    "    ## Encoder \n",
    "    \n",
    "    # Conv2D (input channel, outputchannel, kernel size)\n",
    "    \n",
    "    self.c1 = conv_block(3,16)\n",
    "    self.p1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    self.c2 = conv_block(16,32)\n",
    "    self.p2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "    self.c3 = conv_block(32,64)\n",
    "    self.p3 = nn.MaxPool2d(kernel_size=2, stride=2)  \n",
    "    \n",
    "    self.c4 = conv_block(64,128)\n",
    "    self.p4 = nn.MaxPool2d(kernel_size=2, stride=2)      \n",
    "    \n",
    "    self.c5 = conv_block(128,256)\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "\n",
    "    ## Decoder \n",
    "    \n",
    "    \n",
    "    # Transpose & UpSampling Convblock   \n",
    "    self.t6 = nn.ConvTranspose2d(256,128, kernel_size= 2, stride=2)\n",
    "    self.c6 = conv_block(256,128)\n",
    "\n",
    "    self.t7 = nn.ConvTranspose2d(128,64, kernel_size=2, stride=2)\n",
    "    self.c7 = conv_block(128,64)\n",
    "\n",
    "    self.t8 = nn.ConvTranspose2d(64,32, kernel_size=2, stride=2)\n",
    "    self.c8 = conv_block(64,32)\n",
    "\n",
    "    self.t9 = nn.ConvTranspose2d(32,16, kernel_size=2, stride=2)\n",
    "    self.c9 = conv_block(32,16)\n",
    "    \n",
    "    # Final Classifyer layer \n",
    "    self.outputs = nn.Conv2d(16, n_class, kernel_size= 1)\n",
    "    \n",
    "    #weight initialization\n",
    "\n",
    "    self.c1[0].apply(self.init_weights)\n",
    "    self.c2[0].apply(self.init_weights)\n",
    "    self.c3[0].apply(self.init_weights)\n",
    "    self.c4[0].apply(self.init_weights)\n",
    "    self.c5[0].apply(self.init_weights)\n",
    "    self.c6[0].apply(self.init_weights)\n",
    "    self.c7[0].apply(self.init_weights)\n",
    "    self.c8[0].apply(self.init_weights)\n",
    "    self.c9[0].apply(self.init_weights)\n",
    "    \n",
    "    if cuda: #put the model on the GPU memory\n",
    "      self.cuda()\n",
    "    \n",
    "  def init_weights(self,layer): #gaussian init for the conv layers\n",
    "    nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "    \n",
    "  def forward(self, input):\n",
    "    \"\"\"\n",
    "    the function called to run inference\n",
    "    \"\"\"  \n",
    "    if self.is_cuda: #put data on GPU\n",
    "        input = input.cuda()\n",
    "\n",
    "    # Encoder (Left Side)\n",
    "    c1=self.c1(input)\n",
    "    p1=self.p1(c1)\n",
    "    \n",
    "    c2=self.c2(p1)\n",
    "    p2=self.p2(c2)\n",
    "    \n",
    "    c3=self.c3(p2)\n",
    "    p3=self.p3(c3)\n",
    "    \n",
    "    c4=self.c4(p3)\n",
    "    p4=self.p4(c4)\n",
    "    \n",
    "    c5=self.c5(p4)    \n",
    "    \n",
    "    list_encoder =[c1,p1,c2,p2,c3,p3,c4,p4,c5]\n",
    "\n",
    "    # Decoder (Right Side)\n",
    "    u6=self.t6(c5)\n",
    "    y4 = crop(u6,c4)\n",
    "    concat4 = torch.cat([u6,y4],1)\n",
    "    x6=self.c6(concat4)\n",
    "    \n",
    "    u7=self.t7(x6)\n",
    "    y3 = crop(u7,c3)\n",
    "    x7=self.c7(torch.cat([u7,y3],1))\n",
    "    \n",
    "    u8=self.t8(x7)\n",
    "    y2 = crop(u8,c2)\n",
    "    x8=self.c8(torch.cat([u8,y2],1))\n",
    "    \n",
    "    u9=self.t9(x8)\n",
    "    y1=crop(u9,c1)\n",
    "    \n",
    "    x9=self.c9(torch.cat([u9,y1],1))\n",
    "    \n",
    "    # Final Output Layer\n",
    "    out = self.outputs(x9)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f81d1",
   "metadata": {},
   "source": [
    "### Test du modèle UNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df52cf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred tensor([[[[ 3.2014,  1.7763,  2.2627,  ...,  3.0320,  0.8912,  1.5148],\n",
      "          [ 1.8558,  0.6278,  1.0343,  ...,  1.0752, -0.1118,  0.1783],\n",
      "          [ 2.4056,  1.1724,  0.8346,  ...,  1.6552, -0.5554,  0.7397],\n",
      "          ...,\n",
      "          [ 5.2199,  1.9324,  2.3053,  ...,  2.4124, -1.9416,  1.1826],\n",
      "          [ 4.4953,  1.7722,  1.5551,  ...,  1.7151, -2.0182,  0.4434],\n",
      "          [ 2.9212,  3.2966,  2.9514,  ...,  3.6673,  1.6915,  1.9052]],\n",
      "\n",
      "         [[ 1.4095,  0.8170,  1.0640,  ...,  1.2230,  1.3785,  1.3817],\n",
      "          [ 0.9638, -0.3274, -0.2210,  ..., -0.2285,  0.6141,  0.9847],\n",
      "          [ 0.3233, -0.8271, -0.4823,  ..., -0.8649, -0.3472,  1.2299],\n",
      "          ...,\n",
      "          [ 0.5056, -1.1974, -0.9169,  ..., -0.7821,  0.2010,  1.1125],\n",
      "          [ 1.7131,  1.0656,  0.7498,  ...,  1.0461,  1.2171,  1.9487],\n",
      "          [-0.0276,  1.3139,  1.4973,  ...,  1.8490,  0.7624,  1.3999]]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "output: torch.Size([1, 2, 240, 240])\n"
     ]
    }
   ],
   "source": [
    "img, mask = train_dataset[42]\n",
    "unet = UNet(4,2)\n",
    "pred = unet(img[None,:,:,:]) #the None indicate a batch dimension of 4 N,C,W,H\n",
    "print('pred', pred)\n",
    "print('output:',pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a40730",
   "metadata": {},
   "source": [
    "### Nombre de Paramètres UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa41fcdc",
   "metadata": {},
   "source": [
    "Nous allons calculer le nombre de paramètres pour l'encodeur et décodeur dans UNet, ainsi que son nombre total de paramètre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7d1ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params,table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b830ed74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+\n",
      "|    Modules     | Parameters |\n",
      "+----------------+------------+\n",
      "|  c1.0.weight   |    432     |\n",
      "|   c1.0.bias    |     16     |\n",
      "|  c1.2.weight   |    2304    |\n",
      "|   c1.2.bias    |     16     |\n",
      "|  c2.0.weight   |    4608    |\n",
      "|   c2.0.bias    |     32     |\n",
      "|  c2.2.weight   |    9216    |\n",
      "|   c2.2.bias    |     32     |\n",
      "|  c3.0.weight   |   18432    |\n",
      "|   c3.0.bias    |     64     |\n",
      "|  c3.2.weight   |   36864    |\n",
      "|   c3.2.bias    |     64     |\n",
      "|  c4.0.weight   |   73728    |\n",
      "|   c4.0.bias    |    128     |\n",
      "|  c4.2.weight   |   147456   |\n",
      "|   c4.2.bias    |    128     |\n",
      "|  c5.0.weight   |   294912   |\n",
      "|   c5.0.bias    |    256     |\n",
      "|  c5.2.weight   |   589824   |\n",
      "|   c5.2.bias    |    256     |\n",
      "|   t6.weight    |   131072   |\n",
      "|    t6.bias     |    128     |\n",
      "|  c6.0.weight   |   294912   |\n",
      "|   c6.0.bias    |    128     |\n",
      "|  c6.2.weight   |   147456   |\n",
      "|   c6.2.bias    |    128     |\n",
      "|   t7.weight    |   32768    |\n",
      "|    t7.bias     |     64     |\n",
      "|  c7.0.weight   |   73728    |\n",
      "|   c7.0.bias    |     64     |\n",
      "|  c7.2.weight   |   36864    |\n",
      "|   c7.2.bias    |     64     |\n",
      "|   t8.weight    |    8192    |\n",
      "|    t8.bias     |     32     |\n",
      "|  c8.0.weight   |   18432    |\n",
      "|   c8.0.bias    |     32     |\n",
      "|  c8.2.weight   |    9216    |\n",
      "|   c8.2.bias    |     32     |\n",
      "|   t9.weight    |    2048    |\n",
      "|    t9.bias     |     16     |\n",
      "|  c9.0.weight   |    4608    |\n",
      "|   c9.0.bias    |     16     |\n",
      "|  c9.2.weight   |    2304    |\n",
      "|   c9.2.bias    |     16     |\n",
      "| outputs.weight |     32     |\n",
      "|  outputs.bias  |     2      |\n",
      "+----------------+------------+\n",
      "Total Trainable Params: 1941122\n"
     ]
    }
   ],
   "source": [
    "total_unet, table_unet = count_parameters(unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dac2e72",
   "metadata": {},
   "source": [
    "### Calcul du nombre de paramètres "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100489da",
   "metadata": {},
   "source": [
    "Pour le calcul des paramètres sur chaque convolution, on définit la formule suivante : \n",
    "\n",
    "Conv = (width * height * nombre de filtres dans la couche précédente) * nombre de filtres dans la couche actuelle\n",
    "\n",
    "Dans notre cas (width & height) sont définis dans la kernel size et le nombre de filtres sont les entrées et sorties des canaux dans la conv2d. \n",
    "\n",
    "Par exemple C1 =  Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "On va calculer : $(3*3*3)*16 = 432 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bf8afd",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\"> <b>Nombre de paramètres pour UNet </b></p>\n",
    "\n",
    "| Number  | Modules     | In/Out Channel | # Parameters  |\n",
    "| --------|-----|---------------|-------|\n",
    "| 1 |Conv1.1 (f=3) |(3,16) | 448 |\n",
    "| 2 |Conv1.2 (f=3) |(16,16) | 2320 |\n",
    "| 3 |Conv2.1 (f=3) |(16,32) | 4640 |\n",
    "| 4 |Conv2.2 (f=3) |(32,32) | 9248 |\n",
    "| 5 |Conv3.1 (f=3) |(32,64)| 18464 |\n",
    "| 6 |Conv3.2 (f=3) |(64,64)| 36928 |\n",
    "| 7 |Conv4.1 (f=3) |(64,128)| 73856 |\n",
    "| 8 |Conv4.2 (f=3) |(128,128)| 147584|\n",
    "| 9 |Conv5.1 (f=3) |(128,256)| 295168|\n",
    "| 10 |Conv5.2 (f=3) |(256,256)| 590080|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1aff3",
   "metadata": {},
   "source": [
    "**Nombre de paramètres pour l'encodeur UNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbe98fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_encode_param = [432,16,2304,16,4608,32,9216,32,18432,64,36864,64,73728,128,147456,128,294912,256,589824,256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2883e99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre de paramètre pour l'encodeur unet est 1178768\n"
     ]
    }
   ],
   "source": [
    "print(\"Le nombre de paramètre pour l'encodeur unet est\",sum(unet_encode_param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984887d8",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\"> <b>Nombre de paramètres pour UNet </b></p>\n",
    "\n",
    "| Number  | Modules     | In/Out Channel | # Parameters  |\n",
    "| --------|-----|---------------|-------|\n",
    "| 1 |ConvT6 (f=3) |(256,128) | 131200 |\n",
    "| 2 |Conv6.1 (f=3) |(256,128) | 295040 |\n",
    "| 3 |Conv6.2 (f=3) |(128,128) | 147584 |\n",
    "| 4 |ConvT7 (f=3) |(128,64) | 32832 |\n",
    "| 5 |Conv7.1 (f=3) |(128,64)| 73792 |\n",
    "| 6 |Conv7.2 (f=3) |(64,64)| 36928 |\n",
    "| 7 |ConvT8 (f=3) |(64,32)| 8224 |\n",
    "| 8 |Conv8.1 (f=3) |(64,32)| 18464|\n",
    "| 9 |Conv8.2 (f=3) |(32,32)| 9248|\n",
    "| 10 |ConvT9 (f=3) |(32,16)| 2064|\n",
    "| 11 |Conv9.1 (f=3) |(32,16)|4624|\n",
    "| 12 |Conv9.2 (f=3) |(16,16)|2320|\n",
    "| 13 |Output(f=3) |(16,2)| 34|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1589912",
   "metadata": {},
   "source": [
    "**Nombre de paramètres pour le décodeur unet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6254d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_decode_param = [131072,128,294912,128,147456,128,32768,64,73728,64,36864,64,8192,32,18432,32,9216,32,2048,16,4608,16,2304,16,32,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be518ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre de paramètre pour le décodeur unet est 762354\n"
     ]
    }
   ],
   "source": [
    "print(\"Le nombre de paramètre pour le décodeur unet est\",sum(unet_decode_param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87580986",
   "metadata": {},
   "source": [
    "### Arguments & Hyperparamètres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63d94d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam = {\n",
    "    'lr':0.0001,\n",
    "    'n_epoch':5,\n",
    "    'n_epoch_test':int(5),\n",
    "    'n_class':1,\n",
    "    'batch_size':8,\n",
    "    'n_channel':3,\n",
    "    'conv_width':[16,32,64,128,256,128,64,32,16],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebed56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = (512,512)\n",
    "\n",
    "weights = [0.5, 1.0]\n",
    "class_weights = torch.FloatTensor(weights).cuda()\n",
    "\n",
    "args = {\n",
    "    'nn_loss':nn.BCEWithLogitsLoss(reduction=\"mean\"),\n",
    "    #'nn_loss':nn.CrossEntropyLoss(weight = class_weights,reduction=\"mean\"),\n",
    "    #'nn_loss':BinaryDiceLoss,\n",
    "     'loss_name': 'BinaryCrossentropy',\n",
    "    # 'loss_name': 'Crossentropy',\n",
    "    #'loss_name':'BinaryDiceLoss',\n",
    "    'threshold':0.5,\n",
    "    'cuda':1,\n",
    "    'class_names':['None','Batiment'],\n",
    "    'save_model':False,\n",
    "    'save_model_name':\"unet_test_8_1.pth\",\n",
    "    'train_dataset':InriaDataset(var['variables']['root'],tile_size,'train',None,False,1),\n",
    "    'val_dataset':InriaDataset(var['variables']['root'],tile_size,'validation',None,False,1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707705eb",
   "metadata": {},
   "source": [
    "### Entraînement du Modèle UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a4307f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1941105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40a27ce51134d349ef50c9fd1a722e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None : 87.64%  |  Batiment : 34.56%\n",
      "Epoch   0 -> Train Overall Accuracy: 88.40% Train mIoU : 61.10% Train Loss: 0.2866\n",
      "None : 87.64%  |  Batiment : 34.56%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cae419d10314d0393813754ce4c0833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None : 90.58%  |  Batiment : 52.16%\n",
      "Epoch   1 -> Train Overall Accuracy: 91.46% Train mIoU : 71.37% Train Loss: 0.2061\n",
      "None : 90.58%  |  Batiment : 52.16%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32198fc45034d21a4728c4db39c9335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None : 91.57%  |  Batiment : 57.52%\n",
      "Epoch   2 -> Train Overall Accuracy: 92.44% Train mIoU : 74.54% Train Loss: 0.1838\n",
      "None : 91.57%  |  Batiment : 57.52%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22886a3a3c554b04b51f4289f566fa02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None : 92.24%  |  Batiment : 61.02%\n",
      "Epoch   3 -> Train Overall Accuracy: 93.08% Train mIoU : 76.63% Train Loss: 0.1693\n",
      "None : 92.24%  |  Batiment : 61.02%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deef500a78bb43ce978b12654e55e1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None : 92.73%  |  Batiment : 63.64%\n",
      "Epoch   4 -> Train Overall Accuracy: 93.55% Train mIoU : 78.19% Train Loss: 0.1585\n",
      "None : 92.73%  |  Batiment : 63.64%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96bd05f78a8a4320adcd7b82bb913690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None : 93.27%  |  Batiment : 63.67%\n",
      "Test Overall Accuracy: 93.98% Test mIoU : 78.47%  Test Loss: 0.1479\n",
      "None : 93.27%  |  Batiment : 63.67%\n"
     ]
    }
   ],
   "source": [
    "model = UNet(hparam['n_channel'], hparam['n_class'], cuda=args['cuda'])\n",
    "trained_model, metric_train, metric_test = train_full(args, model,hparam['lr'],hparam['n_epoch'],\n",
    "                                    hparam['n_epoch_test'],hparam['batch_size'],hparam['n_class'],\n",
    "                                    hparam['n_channel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d33166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
