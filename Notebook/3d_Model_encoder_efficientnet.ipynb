{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e28deb4",
   "metadata": {},
   "source": [
    "# Model Encodeur EfficientNet "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a78f49",
   "metadata": {},
   "source": [
    "**Objectif:** le but de ce notebook est d'utilisé l'encodeur EfficientNet et de comprendre les différentes partie de cette encodeur.\n",
    "L'utilité de cet encodeur est le fait qu'il soit plus efficace et plus rapide car il utilie moins de paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84e92e",
   "metadata": {},
   "source": [
    "![title](../img/compare_effnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1eb01",
   "metadata": {},
   "source": [
    "### Root Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646ecc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b1ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/ign.fr/ttea/Code_IGN/AerialImageDataset'\n",
    "train_dir = os.path.join(root,'train/images')\n",
    "gt_dir = os.path.join(root,'train/gt')\n",
    "test_dir = os.path.join(root,'test/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a4c220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37bc8be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '/home/ign.fr/ttea/stage_segmentation_2021/Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "558bfdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.dataloader import InriaDataset\n",
    "from model.model import UNet\n",
    "from train import train_segmentation, eval_segmentation, train_full_segmentation\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from efficientnet_pytorch.utils import url_map, url_map_advprop, get_model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1f5ed9",
   "metadata": {},
   "source": [
    "### Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af58ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ce55123",
   "metadata": {},
   "outputs": [],
   "source": [
    "var= pd.read_json('variables.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f4c69",
   "metadata": {},
   "source": [
    "## Inria Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dcea64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = (512,512)\n",
    "train_dataset = InriaDataset(var['variables']['root'],tile_size,'train',None,False,1)\n",
    "val_dataset = InriaDataset(var['variables']['root'],tile_size,'validation',None,False,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f647b3f",
   "metadata": {},
   "source": [
    "## EfficientNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aaeb1c",
   "metadata": {},
   "source": [
    "On va essayer différentes méthodes pour avoir des modèles plus ou moins efficace en faisant varier certains paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96eb07",
   "metadata": {},
   "source": [
    "Compound Scaling (Mise à l'échelle par composition) : méthode utilisée pour pouvoir scale la largeur, profondeur & résolution ensemble. \n",
    "\n",
    "- **depth :** $d = \\alpha^{\\phi}$  \n",
    "- **width :** $w = \\beta^{\\phi}$\n",
    "- **resolution :** $r = \\gamma^{\\phi}$\n",
    "\n",
    "Ou : $\\alpha * \\beta^2 * \\gamma^2 \\approx = 2$\n",
    "\n",
    "$\\alpha \\geqslant 1, \\beta \\geqslant 1, \\gamma \\geqslant 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1a30e2",
   "metadata": {},
   "source": [
    "$ \\alpha, \\beta, \\gamma$ sont des multiplicateurs d'échelle (scaling multiplier) pour la profondeur, largeur & résolution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b018387a",
   "metadata": {},
   "source": [
    "![title](../img/compoundscaling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd17a52",
   "metadata": {},
   "source": [
    "![title](../img/performancescaling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb66c8c",
   "metadata": {},
   "source": [
    "![title](../img/efficientnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4840526",
   "metadata": {},
   "source": [
    "Sur cette architecture, on peut voir qu'on utilise 7 inverted residual blocks mais chaque block à un setting différent. \n",
    "\n",
    "Ils utilisent aussi le squeeze & excitation block avec la fonction d'activation swish. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b126d7d",
   "metadata": {},
   "source": [
    "### Swish Activation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f48ca",
   "metadata": {},
   "source": [
    "![title](../img/swish.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc27a5",
   "metadata": {},
   "source": [
    "La fonction d'activation ReLU fonctionne plutôt bien mais il reste quelques problèmes au niveau des valeurs négatives et donc leurs dérivées sont toutes égales à 0 lors qu'on utilise des valeurs négatives. \n",
    "\n",
    "Pour palier ce problème, nous allons utiliser une nouvelle fonction d'activation \"Swish\".\n",
    "\n",
    "Swish est une multiplication d'une fonction linéaire et de la sigmoid :\n",
    "- $Swish(x) = x * sigmoid(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a749857",
   "metadata": {},
   "source": [
    "### Inverted Residual Block "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46ffe4c",
   "metadata": {},
   "source": [
    "Le principe d'un bloc résiduel est d'utiliser des convolutions en profondeur puis des faire des convolutions point par point. Cette approche permet de décroître le nombre de paramètres à entraîner. \n",
    "\n",
    "Dans un bloc résiduel original, les skip connections sont utilisées pour connecter une couche large (une couche avec un grand nombre de canaux) et ensuite de prendre quelques canaux à l'intérieur d un block (narrow layers) \n",
    "\n",
    "![title](../img/residualblock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d364b2ac",
   "metadata": {},
   "source": [
    "Le block résidual inversé fait le contraire, c'est-à-dire que les skips connections connectent les couches avec très peu de canaux (narrow layers) pendant que les wider layers sont entre les skips connections \n",
    "\n",
    "![title](../img/invertedblock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d7e37",
   "metadata": {},
   "source": [
    "### Squeeze & Excitation Block "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e98cd",
   "metadata": {},
   "source": [
    "Le bloc de compression et d'excitation (SE) est une méthode pour donner un poids à chaque canal au lieu de les traiter tous de manière égale.\n",
    "\n",
    "SE Block donne en sortie la forme $(1*1*canaux)$ qui spécifie le poids pour chaque canal. L'avantage est que le réseau de neurones peut apprendre ce poids par lui-même comme pour les autres paramètres.\n",
    "\n",
    "![title](../img/SEblock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e89dd09",
   "metadata": {},
   "source": [
    "### MBConv Block  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff456149",
   "metadata": {},
   "source": [
    "MBConv block prends 2 entrées: \n",
    "- La donnée \n",
    "- Block en arguments\n",
    "\n",
    "La donnée est récupérée en sortie de la dernière couche. \n",
    "\n",
    "Le block argument est une collection d'attribut qui peut être utilisée à l'intérieur du MBConv block comme par exemple le nombre d'entrée ou de sortie pour les filtres, taux de dépenses, rapport de compression etc...\n",
    "\n",
    "\n",
    "Dans notre cas, EfficientNet utilise 7 MBConv blocks avec quelques spécifications pour chaque block  :\n",
    "\n",
    "- kernel size pour les convolutions sont 3x3\n",
    "- nombre de répétition qui détermine combien de fois un block en particulier à besoin d'être répétée (doit être plus grand que 0)\n",
    "- Nombre d'entrée et de sortie pour les filtres \n",
    "- Taux de dépenses (expand ratio) filtre d'entrée pour le taux de dépenses \n",
    "- Id skip : détermine si on utilise le skip connection ou non \n",
    "- se ratio : détermine le rapport de compression pour le block SE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6989b4a",
   "metadata": {},
   "source": [
    "### Les Différentes Phases \n",
    "\n",
    "- Expansion Phase \n",
    "- Depthwise Convolution Phase\n",
    "- Squeeze & Excitation Phase \n",
    "- Output Phase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda8fad",
   "metadata": {},
   "source": [
    "### Expansion Phase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f413d5",
   "metadata": {},
   "source": [
    "Nous allons élarger la couche et la rendre plus large (comme on l'a mentionné dans l'inverted residual block) afin d'augmenter le nombre de canaux. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511d3e21",
   "metadata": {},
   "source": [
    "### Dephtwise convolution phase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9cde8e",
   "metadata": {},
   "source": [
    "Après l'élargissement, nous allons utiliser des convolutions en profondeur. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6696e711",
   "metadata": {},
   "source": [
    "### Squeeze & Excitation phase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0138eb",
   "metadata": {},
   "source": [
    "Puis, nous allons extraire les principaux caractéristiques avec Global average pooling & squeeze numbers des canaux en utilisant le SE ratio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b63e8eb",
   "metadata": {},
   "source": [
    "### Output phase "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b26de",
   "metadata": {},
   "source": [
    "Après avoir obtenu le block SE, on applique une opération de convolution qui nous donnera le filtre en sortie. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14289f",
   "metadata": {},
   "source": [
    "### EfficientNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c037176",
   "metadata": {},
   "source": [
    "![title](../img/efficientnet_archi.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a97b01f",
   "metadata": {},
   "source": [
    "**Documentation :**\n",
    "\n",
    "- https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html\n",
    "\n",
    "- https://medium.com/analytics-avidhya/image-classification-with-efficientnet-better-performance-with-computational-efficiency-f480fdb00ac6\n",
    "\n",
    "- https://towardsdatascience.com/mobilenetv2-inverted-residuals-and-linear-bottlenecks-8a4362f4ffd5\n",
    "\n",
    "- https://amaarora.github.io/2020/08/13/efficientnet.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2ac7e4",
   "metadata": {},
   "source": [
    "### Implementation EfficientNet  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749f806",
   "metadata": {},
   "source": [
    "Eff-Unet Paper : https://openaccess.thecvf.com/content_CVPRW_2020/papers/w22/Baheti_Eff-UNet_A_Novel_Architecture_for_Semantic_Segmentation_in_Unstructured_Environment_CVPRW_2020_paper.pdf\n",
    "\n",
    "EfficientUnet Github : https://github.com/zhoudaxia233/EfficientUnet-PyTorch/tree/master/efficientunet\n",
    "\n",
    "https://github.com/lukemelas/EfficientNet-PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f62e982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dReLU(nn.Sequential):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            use_batchnorm=True,\n",
    "    ):\n",
    "\n",
    "        if use_batchnorm == \"inplace\" and InPlaceABN is None:\n",
    "            raise RuntimeError(\n",
    "                \"In order to use `use_batchnorm='inplace'` inplace_abn package must be installed. \"\n",
    "                + \"To install see: https://github.com/mapillary/inplace_abn\"\n",
    "            )\n",
    "\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        if use_batchnorm == \"inplace\":\n",
    "            bn = InPlaceABN(out_channels, activation=\"leaky_relu\", activation_param=0.0)\n",
    "            relu = nn.Identity()\n",
    "\n",
    "        elif use_batchnorm and use_batchnorm != \"inplace\":\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        else:\n",
    "            bn = nn.Identity()\n",
    "\n",
    "        super(Conv2dReLU, self).__init__(conv, bn, relu)\n",
    "\n",
    "\n",
    "class SCSEModule(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.cSE = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.cSE(x) + x * self.sSE(x)\n",
    "\n",
    "\n",
    "class ArgMax(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.argmax(x, dim=self.dim)\n",
    "\n",
    "\n",
    "class Activation(nn.Module):\n",
    "\n",
    "    def __init__(self, name, **params):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if name is None or name == 'identity':\n",
    "            self.activation = nn.Identity(**params)\n",
    "        elif name == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif name == 'softmax2d':\n",
    "            self.activation = nn.Softmax(dim=1, **params)\n",
    "        elif name == 'softmax':\n",
    "            self.activation = nn.Softmax(**params)\n",
    "        elif name == 'logsoftmax':\n",
    "            self.activation = nn.LogSoftmax(**params)\n",
    "        elif name == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif name == 'argmax':\n",
    "            self.activation = ArgMax(**params)\n",
    "        elif name == 'argmax2d':\n",
    "            self.activation = ArgMax(dim=1, **params)\n",
    "        elif callable(name):\n",
    "            self.activation = name(**params)\n",
    "        else:\n",
    "            raise ValueError('Activation should be callable/sigmoid/softmax/logsoftmax/tanh/None; got {}'.format(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, name, **params):\n",
    "        super().__init__()\n",
    "\n",
    "        if name is None:\n",
    "            self.attention = nn.Identity(**params)\n",
    "        elif name == 'scse':\n",
    "            self.attention = SCSEModule(**params)\n",
    "        else:\n",
    "            raise ValueError(\"Attention {} is not implemented\".format(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attention(x)\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fdb6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderMixin:\n",
    "    \"\"\"Add encoder functionality such as:\n",
    "        - output channels specification of feature tensors (produced by encoder)\n",
    "        - patching first convolution for arbitrary input channels\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def out_channels(self):\n",
    "        \"\"\"Return channels dimensions for each tensor of forward output of encoder\"\"\"\n",
    "        return self._out_channels[: self._depth + 1]\n",
    "\n",
    "    def set_in_channels(self, in_channels, pretrained=True):\n",
    "        \"\"\"Change first convolution channels\"\"\"\n",
    "        if in_channels == 3:\n",
    "            return\n",
    "\n",
    "        self._in_channels = in_channels\n",
    "        if self._out_channels[0] == 3:\n",
    "            self._out_channels = tuple([in_channels] + list(self._out_channels)[1:])\n",
    "\n",
    "        utils.patch_first_conv(model=self, new_in_channels=in_channels, pretrained=pretrained)\n",
    "\n",
    "    def get_stages(self):\n",
    "        \"\"\"Method should be overridden in encoder\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def make_dilated(self, output_stride):\n",
    "\n",
    "        if output_stride == 16:\n",
    "            stage_list=[5,]\n",
    "            dilation_list=[2,]\n",
    "            \n",
    "        elif output_stride == 8:\n",
    "            stage_list=[4, 5]\n",
    "            dilation_list=[2, 4] \n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Output stride should be 16 or 8, got {}.\".format(output_stride))\n",
    "        \n",
    "        stages = self.get_stages()\n",
    "        for stage_indx, dilation_rate in zip(stage_list, dilation_list):\n",
    "            utils.replace_strides_with_dilation(\n",
    "                module=stages[stage_indx],\n",
    "                dilation_rate=dilation_rate,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e2c1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetEncoder(EfficientNet, EncoderMixin):\n",
    "    def __init__(self, stage_idxs, out_channels, model_name, depth=5):\n",
    "\n",
    "        blocks_args, global_params = get_model_params(model_name, override_params=None)\n",
    "        super().__init__(blocks_args, global_params)\n",
    "        #super().__init__()\n",
    "        self._stage_idxs = stage_idxs\n",
    "        self._out_channels = out_channels\n",
    "        self._depth = depth\n",
    "        self._in_channels = 3\n",
    "\n",
    "        del self._fc\n",
    "\n",
    "    def get_stages(self):\n",
    "        return [\n",
    "            nn.Identity(),\n",
    "            nn.Sequential(self._conv_stem, self._bn0, self._swish),\n",
    "            self._blocks[:self._stage_idxs[0]],\n",
    "            self._blocks[self._stage_idxs[0]:self._stage_idxs[1]],\n",
    "            self._blocks[self._stage_idxs[1]:self._stage_idxs[2]],\n",
    "            self._blocks[self._stage_idxs[2]:],\n",
    "        ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        stages = self.get_stages()\n",
    "\n",
    "        block_number = 0.\n",
    "        drop_connect_rate = self._global_params.drop_connect_rate\n",
    "        #drop_connect_rate  = 0.2\n",
    "        features = []\n",
    "        for i in range(self._depth + 1):\n",
    "\n",
    "            # Identity and Sequential stages\n",
    "            if i < 2:\n",
    "                x = stages[i](x)\n",
    "\n",
    "            # Block stages need drop_connect rate\n",
    "            else:\n",
    "                for module in stages[i]:\n",
    "                    drop_connect = drop_connect_rate * block_number / len(self._blocks)\n",
    "                    block_number += 1.\n",
    "                    x = module(x, drop_connect)\n",
    "\n",
    "            features.append(x)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def load_state_dict(self, state_dict, **kwargs):\n",
    "        state_dict.pop(\"_fc.bias\", None)\n",
    "        state_dict.pop(\"_fc.weight\", None)\n",
    "        super().load_state_dict(state_dict, **kwargs)\n",
    "\n",
    "\n",
    "def _get_pretrained_settings(encoder):\n",
    "    pretrained_settings = {\n",
    "        \"imagenet\": {\n",
    "            \"mean\": [0.485, 0.456, 0.406],\n",
    "            \"std\": [0.229, 0.224, 0.225],\n",
    "            \"url\": url_map[encoder],\n",
    "            \"input_space\": \"RGB\",\n",
    "            \"input_range\": [0, 1],\n",
    "        },\n",
    "        \"advprop\": {\n",
    "            \"mean\": [0.5, 0.5, 0.5],\n",
    "            \"std\": [0.5, 0.5, 0.5],\n",
    "            \"url\": url_map_advprop[encoder],\n",
    "            \"input_space\": \"RGB\",\n",
    "            \"input_range\": [0, 1],\n",
    "        }\n",
    "    }\n",
    "    return pretrained_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35168871",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, mask = train_dataset[42]\n",
    "Encoder = EfficientNetEncoder((3,5,9,16),[3,32,24,40,112,320],\"efficientnet-b0\")\n",
    "encoder = Encoder(img[None,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613cda89",
   "metadata": {},
   "source": [
    "## Import EfficientNet Segmentation Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d301bbd",
   "metadata": {},
   "source": [
    "https://github.com/qubvel/segmentation_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a6ef0d",
   "metadata": {},
   "source": [
    "### EffificentNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "143dad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet = smp.Unet(\n",
    "    encoder_name=\"efficientnet-b0\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=2,                      # model output channels (number of classes in your dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a14329",
   "metadata": {},
   "source": [
    "### Arguments & Hyperparamètres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b48e9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparam = {\n",
    "    'lr':0.0001,\n",
    "    'n_epoch':5,\n",
    "    'n_epoch_test':int(5),\n",
    "    'n_class':1,\n",
    "    'batch_size':8,\n",
    "    'n_channel':3,\n",
    "    'conv_width':[16,32,64,128,256,128,64,32,16],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1faa1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = (512,512)\n",
    "\n",
    "weights = [0.5, 1.0]\n",
    "class_weights = torch.FloatTensor(weights).cuda()\n",
    "\n",
    "args = {\n",
    "    'nn_loss':nn.BCEWithLogitsLoss(reduction=\"mean\"),\n",
    "    #'nn_loss':nn.CrossEntropyLoss(weight = class_weights,reduction=\"mean\"),\n",
    "    #'nn_loss':BinaryDiceLoss,\n",
    "    #'loss_name':'BinaryDiceLoss',\n",
    "     'loss_name': 'BinaryCrossentropy',\n",
    "    # 'loss_name': 'Crossentropy',\n",
    "    'threshold':0.5,\n",
    "    'cuda':1,\n",
    "    'class_names':['None','Batiment'],\n",
    "    'save_model':False,\n",
    "    'save_model_name':\"unet_test_8_1.pth\",\n",
    "    'train_dataset':InriaDataset(var['variables']['root'],tile_size,'train',None,False,1),\n",
    "    'val_dataset':InriaDataset(var['variables']['root'],tile_size,'validation',None,False,1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e74e59d",
   "metadata": {},
   "source": [
    "### Training EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a5d3492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trained_model, metric_train,metric_test = train_full(args, efficientnet,hparam['lr'],hparam['n_epoch'],\n",
    "#                                    hparam['n_epoch_test'],hparam['batch_size'],hparam['n_class'],\n",
    "#                                    hparam['n_channel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d7b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
